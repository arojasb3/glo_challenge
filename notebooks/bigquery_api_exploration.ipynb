{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4ca723",
   "metadata": {},
   "source": [
    "## BigQuery Write API\n",
    "On this notebook we will try the bigquery write api, so we can be ready to send new data to our tables.\n",
    "\n",
    "For that we created a .proto file that can be seen on the api/proto_message.proto address.\n",
    "\n",
    "and ran the following command \n",
    "\n",
    "```protoc -I=. --python_out=. ./proto_message.proto```\n",
    "\n",
    "This created a .py file that can be seen on the repo. We will now use that generated file to create and parse our messages sent to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38de085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os, json, uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058c8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import proto_message_pb2 as bqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4ad2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = bqm.Job()\n",
    "job.id = -1\n",
    "job.job = \"Not registered job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6ddea86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "department = bqm.Department()\n",
    "department.id = -1\n",
    "department.department = \"Not registered department\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18b8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "hired_employee = bqm.Hired_employee()\n",
    "hired_employee.id = 2000\n",
    "hired_employee.name = \"Alejandro Rojas\"\n",
    "hired_employee.datetime = datetime.datetime(2023,2,20,19).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "hired_employee.department_id = 5\n",
    "hired_employee.job_id = 179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62afe07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676937912.000034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " datetime.datetime(2023,2,20,19,5,12,34).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0887c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x08\\xd0\\x0f\\x12\\x0fAlejandro Rojas\\x1a\\x132023-02-20 19:00:00 \\x05(\\xb3\\x01'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hired_employee.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602003f9",
   "metadata": {},
   "source": [
    "Great! We've created our protobuf messages. \n",
    "\n",
    "Now, following a kind guide written by matthieucham from stax labs, we will try to use a wrapper for our BigQuery calls. Initially try the one suggested by him, if not then tweak it\n",
    "\n",
    "https://dev.to/stack-labs/13-tricks-for-the-new-bigquery-storage-write-api-in-python-296e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d1d7fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DescriptorProto in module google.protobuf.descriptor_pb2:\n",
      "\n",
      "class DescriptorProto(google.protobuf.pyext._message.CMessage, google.protobuf.message.Message)\n",
      " |  A ProtocolMessage\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DescriptorProto\n",
      " |      google.protobuf.pyext._message.CMessage\n",
      " |      google.protobuf.message.Message\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  enum_type\n",
      " |      Field google.protobuf.DescriptorProto.enum_type\n",
      " |  \n",
      " |  extension\n",
      " |      Field google.protobuf.DescriptorProto.extension\n",
      " |  \n",
      " |  extension_range\n",
      " |      Field google.protobuf.DescriptorProto.extension_range\n",
      " |  \n",
      " |  field\n",
      " |      Field google.protobuf.DescriptorProto.field\n",
      " |  \n",
      " |  name\n",
      " |      Field google.protobuf.DescriptorProto.name\n",
      " |  \n",
      " |  nested_type\n",
      " |      Field google.protobuf.DescriptorProto.nested_type\n",
      " |  \n",
      " |  oneof_decl\n",
      " |      Field google.protobuf.DescriptorProto.oneof_decl\n",
      " |  \n",
      " |  options\n",
      " |      Field google.protobuf.DescriptorProto.options\n",
      " |  \n",
      " |  reserved_name\n",
      " |      Field google.protobuf.DescriptorProto.reserved_name\n",
      " |  \n",
      " |  reserved_range\n",
      " |      Field google.protobuf.DescriptorProto.reserved_range\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  DESCRIPTOR = <google.protobuf.pyext._message.MessageDescriptor object>\n",
      " |  \n",
      " |  ExtensionRange = <class 'google.protobuf.descriptor_pb2.ExtensionRange...\n",
      " |      A ProtocolMessage\n",
      " |  \n",
      " |  ReservedRange = <class 'google.protobuf.descriptor_pb2.ReservedRange'>\n",
      " |      A ProtocolMessage\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  ByteSize(...)\n",
      " |      Returns the size of the message in bytes.\n",
      " |  \n",
      " |  Clear(...)\n",
      " |      Clears the message.\n",
      " |  \n",
      " |  ClearExtension(...)\n",
      " |      Clears a message field.\n",
      " |  \n",
      " |  ClearField(...)\n",
      " |      Clears a message field.\n",
      " |  \n",
      " |  CopyFrom(...)\n",
      " |      Copies a protocol message into the current message.\n",
      " |  \n",
      " |  DiscardUnknownFields(...)\n",
      " |      Discards the unknown fields.\n",
      " |  \n",
      " |  FindInitializationErrors(...)\n",
      " |      Finds unset required fields.\n",
      " |  \n",
      " |  HasExtension(...)\n",
      " |      Checks if a message field is set.\n",
      " |  \n",
      " |  HasField(...)\n",
      " |      Checks if a message field is set.\n",
      " |  \n",
      " |  IsInitialized(...)\n",
      " |      Checks if all required fields of a protocol message are set.\n",
      " |  \n",
      " |  ListFields(...)\n",
      " |      Lists all set fields of a message.\n",
      " |  \n",
      " |  MergeFrom(...)\n",
      " |      Merges a protocol message into the current message.\n",
      " |  \n",
      " |  MergeFromString(...)\n",
      " |      Merges a serialized message into the current message.\n",
      " |  \n",
      " |  ParseFromString(...)\n",
      " |      Parses a serialized message into the current message.\n",
      " |  \n",
      " |  SerializePartialToString(...)\n",
      " |      Serializes the message to a string, even if it isn't initialized.\n",
      " |  \n",
      " |  SerializeToString(...)\n",
      " |      Serializes the message to a string, only for initialized messages.\n",
      " |  \n",
      " |  SetInParent(...)\n",
      " |      Sets the has bit of the given field in its parent message.\n",
      " |  \n",
      " |  UnknownFields(...)\n",
      " |      Parse unknown field set\n",
      " |  \n",
      " |  WhichOneof(...)\n",
      " |      Returns the name of the field set inside a oneof, or None if no field is set.\n",
      " |  \n",
      " |  __deepcopy__(...)\n",
      " |      Makes a deep copy of the class.\n",
      " |  \n",
      " |  __eq__(self, value, /)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __ge__(self, value, /)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __gt__(self, value, /)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self, /)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __le__(self, value, /)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __lt__(self, value, /)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, value, /)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |      Outputs a unicode representation of the message.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  FromString(...) from google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\n",
      " |      Creates new method instance from given serialized data.\n",
      " |  \n",
      " |  RegisterExtension(...) from google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\n",
      " |      Registers an extension with the current message.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from google.protobuf.pyext._message.MessageMeta\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from google.protobuf.pyext._message.CMessage:\n",
      " |  \n",
      " |  Extensions\n",
      " |      Extension dict\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.protobuf.message.Message:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Support the pickle protocol.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |      Support the pickle protocol.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(descriptor_pb2.DescriptorProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59f81c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function CopyToProto:\n",
      "\n",
      "CopyToProto(...) method of google.protobuf.pyext._message.FileDescriptor instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bqm.DESCRIPTOR.CopyToProto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec781d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around BigQuery call.\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Any, Iterable\n",
    "import logging\n",
    "from google.cloud import bigquery_storage\n",
    "from google.cloud.bigquery_storage_v1 import exceptions as bqstorage_exceptions\n",
    "\n",
    "from google.cloud.bigquery_storage_v1 import types, writer\n",
    "from google.protobuf import descriptor_pb2\n",
    "from google.protobuf.descriptor import Descriptor\n",
    "\n",
    "\n",
    "\n",
    "class DefaultStreamManager:  # pragma: no cover\n",
    "    \"\"\"Manage access to the _default stream write streams.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        table_path: str,\n",
    "        message_protobuf_descriptor: Descriptor,\n",
    "        bigquery_storage_write_client: bigquery_storage.BigQueryWriteClient,\n",
    "    ):\n",
    "        \"\"\"Init.\"\"\"\n",
    "        self.stream_name = f\"{table_path}/_default\"\n",
    "        self.message_protobuf_descriptor = message_protobuf_descriptor\n",
    "        self.write_client = bigquery_storage_write_client\n",
    "        self.append_rows_stream = None\n",
    "\n",
    "    def _init_stream(self):\n",
    "        \"\"\"Init the underlying stream manager.\"\"\"\n",
    "        # Create a template with fields needed for the first request.\n",
    "        request_template = types.AppendRowsRequest()\n",
    "        # The initial request must contain the stream name.\n",
    "        request_template.write_stream = self.stream_name\n",
    "        # So that BigQuery knows how to parse the serialized_rows, generate a\n",
    "        # protocol buffer representation of our message descriptor.\n",
    "        proto_schema = types.ProtoSchema()\n",
    "        proto_descriptor = descriptor_pb2.DescriptorProto()  # pylint: disable=no-member\n",
    "        self.message_protobuf_descriptor.CopyToProto(proto_descriptor)\n",
    "        proto_schema.proto_descriptor = proto_descriptor\n",
    "        proto_data = types.AppendRowsRequest.ProtoData()\n",
    "        proto_data.writer_schema = proto_schema\n",
    "        request_template.proto_rows = proto_data\n",
    "        # Create an AppendRowsStream using the request template created above.\n",
    "        self.append_rows_stream = writer.AppendRowsStream(\n",
    "            self.write_client, request_template\n",
    "        )\n",
    "\n",
    "    def send_appendrowsrequest(\n",
    "        self, request: types.AppendRowsRequest\n",
    "    ) -> writer.AppendRowsFuture:\n",
    "        \"\"\"Send request to the stream manager. Init the stream manager if needed.\"\"\"\n",
    "        try:\n",
    "            if self.append_rows_stream is None:\n",
    "                self._init_stream()\n",
    "            return self.append_rows_stream.send(request)\n",
    "        except bqstorage_exceptions.StreamClosedError:\n",
    "            # the stream needs to be reinitialized\n",
    "            self.append_rows_stream.close()\n",
    "            self.append_rows_stream = None\n",
    "            raise\n",
    "\n",
    "    # Use as a context manager\n",
    "\n",
    "    def __enter__(self) -> DefaultStreamManager:\n",
    "        \"\"\"Enter the context manager. Return the stream name.\"\"\"\n",
    "        self._init_stream()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Exit the context manager : close the stream.\"\"\"\n",
    "        if self.append_rows_stream is not None:\n",
    "            # Shutdown background threads and close the streaming connection.\n",
    "            self.append_rows_stream.close()\n",
    "\n",
    "\n",
    "class BigqueryWriteManager:\n",
    "    \"\"\"Encapsulation for bigquery client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        table_id: str,\n",
    "        bigquery_storage_write_client: bigquery_storage.BigQueryWriteClient,\n",
    "        pb2_descriptor: Descriptor,\n",
    "    ):  # pragma: no cover\n",
    "        \"\"\"Create a BigQueryManager.\"\"\"\n",
    "        self.bigquery_storage_write_client = bigquery_storage_write_client\n",
    "\n",
    "        self.table_path = self.bigquery_storage_write_client.table_path(\n",
    "            project_id, dataset_id, table_id\n",
    "        )\n",
    "        self.pb2_descriptor = pb2_descriptor\n",
    "\n",
    "    def write_rows(self, pb_rows: Iterable[Any]) -> None:\n",
    "        \"\"\"Write data rows.\"\"\"\n",
    "        with DefaultStreamManager(\n",
    "            self.table_path, self.pb2_descriptor, self.bigquery_storage_write_client\n",
    "        ) as target_stream_manager:\n",
    "            proto_rows = types.ProtoRows()\n",
    "            # Create a batch of row data by appending proto2 serialized bytes to the\n",
    "            # serialized_rows repeated field.\n",
    "            for row in pb_rows:\n",
    "                proto_rows.serialized_rows.append(row.SerializeToString())\n",
    "            # Create an append row request containing the rows\n",
    "            request = types.AppendRowsRequest()\n",
    "            proto_data = types.AppendRowsRequest.ProtoData()\n",
    "            proto_data.rows = proto_rows\n",
    "            request.proto_rows = proto_data\n",
    "\n",
    "            future = target_stream_manager.send_appendrowsrequest(request)\n",
    "\n",
    "            # Wait for the append row requests to finish.\n",
    "            print(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea7e3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (\n",
    "    BigqueryWriteManager(\n",
    "        project_id = os.environ['GCP_PROJECT'],\n",
    "        dataset_id = 'globant',\n",
    "        table_id = 'departments',\n",
    "        bigquery_storage_write_client = bigquery_storage.BigQueryWriteClient(),\n",
    "        pb2_descriptor = bqm.Job.DESCRIPTOR\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cced9ff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unknown",
     "evalue": "None There was a problem opening the stream. Try turning on DEBUG level logs to see the error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknown\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdepartment\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 112\u001b[0m, in \u001b[0;36mBigqueryWriteManager.write_rows\u001b[1;34m(self, pb_rows)\u001b[0m\n\u001b[0;32m    109\u001b[0m proto_data\u001b[38;5;241m.\u001b[39mrows \u001b[38;5;241m=\u001b[39m proto_rows\n\u001b[0;32m    110\u001b[0m request\u001b[38;5;241m.\u001b[39mproto_rows \u001b[38;5;241m=\u001b[39m proto_data\n\u001b[1;32m--> 112\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_stream_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_appendrowsrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Wait for the append row requests to finish.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(future\u001b[38;5;241m.\u001b[39mresult())\n",
      "Cell \u001b[1;32mIn[52], line 56\u001b[0m, in \u001b[0;36mDefaultStreamManager.send_appendrowsrequest\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_rows_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_stream()\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_rows_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m bqstorage_exceptions\u001b[38;5;241m.\u001b[39mStreamClosedError:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# the stream needs to be reinitialized\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend_rows_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\globant\\lib\\site-packages\\google\\cloud\\bigquery_storage_v1\\writer.py:234\u001b[0m, in \u001b[0;36mAppendRowsStream.send\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opening:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_active:\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# For each request, we expect exactly one response (in order). Add a\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# future to the queue so that when the response comes, the callback can\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# pull it off and notify completion.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m future \u001b[38;5;241m=\u001b[39m AppendRowsFuture(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\globant\\lib\\site-packages\\google\\cloud\\bigquery_storage_v1\\writer.py:207\u001b[0m, in \u001b[0;36mAppendRowsStream._open\u001b[1;34m(self, initial_request, timeout)\u001b[0m\n\u001b[0;32m    202\u001b[0m     request_exception \u001b[38;5;241m=\u001b[39m exceptions\u001b[38;5;241m.\u001b[39mUnknown(\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a problem opening the stream. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry turning on DEBUG level logs to see the error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m     )\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose(reason\u001b[38;5;241m=\u001b[39mrequest_exception)\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m request_exception\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inital_response_future\n",
      "\u001b[1;31mUnknown\u001b[0m: None There was a problem opening the stream. Try turning on DEBUG level logs to see the error."
     ]
    }
   ],
   "source": [
    "test.write_rows([department])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09af0f1c",
   "metadata": {},
   "source": [
    "Perfect! Now that we managed to write data into BigQuery we need to create a function that can validate the data incoming\n",
    "before actually writing it into de Data Base. \n",
    "\n",
    "For that we will use out new protobuf knowledge, and letting them handle any errors regarding the wrong records.\n",
    "\n",
    "\n",
    "One more thing before I forget it! \n",
    "We will create a logs table with every error, so we also have to modify our .proto file for a forth schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d0d11",
   "metadata": {},
   "source": [
    "This is an ok request for our jobs table, so let's try to parse it into a protobuf list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df4a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_message(req, proto_class):\n",
    "    proto = proto_class()\n",
    "    for attributes in req.keys():\n",
    "        try:\n",
    "            setattr(proto, attributes, req[attributes])\n",
    "        except AttributeError as e:\n",
    "            raise e \n",
    "        except TypeError as e:\n",
    "            raise e \n",
    "    return proto\n",
    "\n",
    "def parse_request(request, proto_class):\n",
    "    json_request = request.get_json()\n",
    "    try:\n",
    "        current_record = 1\n",
    "        parsed_request = []\n",
    "        for row in json_request:\n",
    "            parsed_request.append(parse_message(row, proto_class))\n",
    "            current_record = current_record + 1\n",
    "        return parsed_request\n",
    "    except AttributeError as e:\n",
    "        error_string = str(e)\n",
    "        return ({\"msg\": f\"{error_string} on record number {current_record}\"}), 400  \n",
    "    except TypeError as e:\n",
    "        error_string = str(e)\n",
    "        return ({\"msg\": f\"{error_string} on record number {current_record}\"}), 400  \n",
    "        \n",
    "class Dummy_request():\n",
    "    def __init__(self, json_request):\n",
    "        self.json_request = json_request\n",
    "    \n",
    "    def get_json(self):\n",
    "        return self.json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da6283f",
   "metadata": {},
   "source": [
    "We are handling 2 types of errors here:\n",
    "- AttributeError when users send a key that does not belong to the dataset schema\n",
    "- TypeError when the type of a request does not match with the type registered at the proto file.\n",
    "\n",
    "Let's try a correct request and see if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed7830d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_job_request = Dummy_request([\n",
    "    {'id': 184, 'job': 'Test Job'},\n",
    "    {'id': 185, 'job': 'Test Job'},\n",
    "    {'id': 186, 'job': 'Test Job'},\n",
    "    {'id': 187, 'job': 'Test Job'},\n",
    "    {'id': 188, 'job': 'Test Job'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0178f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_request = parse_request(good_job_request, bqm.Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a3e6793f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: 184\n",
       " job: \"Test Job\",\n",
       " id: 185\n",
       " job: \"Test Job\",\n",
       " id: 186\n",
       " job: \"Test Job\",\n",
       " id: 187\n",
       " job: \"Test Job\",\n",
       " id: 188\n",
       " job: \"Test Job\"]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9448e08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[proto_message_pb2.Job,\n",
       " proto_message_pb2.Job,\n",
       " proto_message_pb2.Job,\n",
       " proto_message_pb2.Job,\n",
       " proto_message_pb2.Job]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[type(x) for x in parsed_request]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18caa1a",
   "metadata": {},
   "source": [
    "Great!\n",
    "We are parsing just fine a list of dictionaries.\n",
    "Now let's try a couple of bad requests and see what happens.\n",
    "\n",
    "Off course if we get 2 different errors, the function will return the first one to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f266f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_job_request_attr_1 = Dummy_request([\n",
    "    {'id': 184, 'job': 'Test Job'},\n",
    "    {'id': 185, 'job': 'Test Job'},\n",
    "    {'id': 186, 'job': 'Test Job'},\n",
    "    {'idi': 187, 'job': 'Test Job'},\n",
    "    {'id': 188, 'job': 'Test Job'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4978329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_parsed_request_1 = parse_request(bad_job_request_attr_1, bqm.Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "356e7b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'msg': \"'Job' object has no attribute 'idi' on record number 4\"}, 400)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_parsed_request_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf058b3",
   "metadata": {},
   "source": [
    "Now one with the wrong type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "52fe510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_job_request_attr_2 = Dummy_request([\n",
    "    {'id': 184, 'job': 'Test Job'},\n",
    "    {'id': 185, 'job': 'Test Job'},\n",
    "    {'id': '186', 'job': 'Test Job'},\n",
    "    {'id': 187, 'job': 'Test Job'},\n",
    "    {'id': 188, 'job': 'Test Job'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bc654089",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_parsed_request_2 = parse_request(bad_job_request_attr_2, bqm.Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4af49943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'msg': \"'186' has type str, but expected one of: int on record number 3\"},\n",
       " 400)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_parsed_request_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281fbab3",
   "metadata": {},
   "source": [
    "We are back,\n",
    "This time we don't want to send an error message and stop the process when we find a bad record. We will instead save it into a bad records list, which will be uploaded later into the bad records table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1eb55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_message(req, proto_class):\n",
    "    proto = proto_class()\n",
    "    for attributes in req.keys():\n",
    "        try:\n",
    "            setattr(proto, attributes, req[attributes])\n",
    "        except AttributeError as e:\n",
    "            raise e \n",
    "        except TypeError as e:\n",
    "            raise e \n",
    "    return proto\n",
    "\n",
    "def parse_wrong_record(req, error_class, table_name, error, datetime_utc):\n",
    "    err = error_class()\n",
    "    setattr(err, 'uuid', str(uuid.uuid4()))\n",
    "    setattr(err,'content', json.dumps(req) )\n",
    "    setattr(err,'table', table_name )\n",
    "    setattr(err,'datetime', datetime_utc )\n",
    "    setattr(err,'error', error )\n",
    "    return err\n",
    "    \n",
    "    \n",
    "def parse_request(request, proto_class, table_name):\n",
    "    json_request = request.get_json()\n",
    "    parsed_request = []\n",
    "    bad_requests = []\n",
    "    current_time = datetime.datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    current_record = 1\n",
    "    for row in json_request:\n",
    "        try:\n",
    "            parsed_request.append(parse_message(row, proto_class))\n",
    "        except AttributeError as e:\n",
    "            error_string = str(e)\n",
    "            bad_requests.append(\n",
    "                parse_wrong_record(\n",
    "                    row,\n",
    "                    bqm.Wrong_record,\n",
    "                    table_name,\n",
    "                    f\"{error_string} on record number {current_record}\",\n",
    "                    current_time\n",
    "                )\n",
    "            )\n",
    "            #return ({\"msg\": f\"{error_string} on record number {current_record}\"}), 400  \n",
    "        except TypeError as e:\n",
    "            error_string = str(e)\n",
    "            bad_requests.append(\n",
    "                parse_wrong_record(\n",
    "                    row,\n",
    "                    bqm.Wrong_record,\n",
    "                    table_name,\n",
    "                    f\"{error_string} on record number {current_record}\",\n",
    "                    current_time\n",
    "                )\n",
    "            )\n",
    "            #return ({\"msg\": f\"{error_string} on record number {current_record}\"}), 400\n",
    "        \n",
    "        current_record = current_record + 1\n",
    "    return parsed_request, bad_requests\n",
    "        \n",
    "class Dummy_request():\n",
    "    def __init__(self, json_request):\n",
    "        self.json_request = json_request\n",
    "    \n",
    "    def get_json(self):\n",
    "        return self.json_request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bda50",
   "metadata": {},
   "source": [
    "Let's try the following list with 2 bad records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c13743",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_job_request_2_rec = Dummy_request([\n",
    "    {'id': 184, 'job': 'Test Job'},\n",
    "    {'id': '185', 'job': 'Test Job'},\n",
    "    {'id': 186, 'job': 'Test Job'},\n",
    "    {'idi': 187, 'job': 'Test Job'},\n",
    "    {'id': 188, 'job': 'Test Job'}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c0ee8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_parse_request = parse_request(bad_job_request_2_rec, bqm.Job, 'Jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d3a3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([id: 184\n",
       "  job: \"Test Job\",\n",
       "  id: 186\n",
       "  job: \"Test Job\",\n",
       "  id: 188\n",
       "  job: \"Test Job\"],\n",
       " [uuid: \"de04f339-bd2c-4d98-aea9-10c1f13bede5\"\n",
       "  content: \"{\\\"id\\\": \\\"185\\\", \\\"job\\\": \\\"Test Job\\\"}\"\n",
       "  table: \"Jobs\"\n",
       "  error: \"\\'185\\' has type str, but expected one of: int on record number 2\"\n",
       "  datetime: \"2023-02-15 01:47:50\",\n",
       "  uuid: \"fda633f3-d6a7-4603-b8a1-d342da56c75e\"\n",
       "  content: \"{\\\"idi\\\": 187, \\\"job\\\": \\\"Test Job\\\"}\"\n",
       "  table: \"Jobs\"\n",
       "  error: \"\\'Job\\' object has no attribute \\'idi\\' on record number 4\"\n",
       "  datetime: \"2023-02-15 01:47:50\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71673a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'95545431-d156-41a3-9176-6b95a47946c0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0fd0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('723709d2-6f4f-452b-a833-a12b66f7fe39')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request[1][0].uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15266197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"idi\": 187, \"job\": \"Test Job\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request[1][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05a0a55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jobs'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request[1][0].table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bdc8873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Job' object has no attribute 'idi' on record number 4\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request[1][0].error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7146e232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-14 23:56:17'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_parse_request[1][0].datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed76f2",
   "metadata": {},
   "source": [
    "This looks good. I think we are ready to set up our function inside our api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4476e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"id\": 184, \"job\": \"Test Job\"}, {\"id\": \"185\", \"job\": \"Test Job\"}, {\"id\": 186, \"job\": \"Test Job\"}, {\"idi\": 187, \"job\": \"Test Job\"}, {\"id\": 188, \"job\": \"Test Job\"}]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(bad_job_request_2_rec.get_json()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14118b67",
   "metadata": {},
   "source": [
    "Just before we start testing our API, let's try the entire flow of functions and check if they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d29263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = (\n",
    "    BigqueryWriteManager(\n",
    "        project_id = os.environ['GCP_PROJECT'],\n",
    "        dataset_id = 'globant',\n",
    "        table_id = 'jobs',\n",
    "        bigquery_storage_write_client = bigquery_storage.BigQueryWriteClient(),\n",
    "        pb2_descriptor = bqm.Job.DESCRIPTOR\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c490fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append_result {\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_test.write_rows(\n",
    "    mixed_parse_request[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6928b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_bad = (\n",
    "    BigqueryWriteManager(\n",
    "        project_id = os.environ['GCP_PROJECT'],\n",
    "        dataset_id = 'globant',\n",
    "        table_id = 'error_logs',\n",
    "        bigquery_storage_write_client = bigquery_storage.BigQueryWriteClient(),\n",
    "        pb2_descriptor = bqm.Wrong_record.DESCRIPTOR\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7985698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append_result {\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_test_bad.write_rows(\n",
    "    mixed_parse_request[1]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
