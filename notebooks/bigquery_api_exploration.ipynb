{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab4ca723",
   "metadata": {},
   "source": [
    "## BigQuery Write API\n",
    "On this notebook we will try the bigquery write api, so we can be ready to send new data to our tables.\n",
    "\n",
    "For that we created a .proto file that can be seen on the api/proto_message.proto address.\n",
    "\n",
    "and ran the following command \n",
    "\n",
    "```protoc -I=. --python_out=. ./proto_message.proto```\n",
    "\n",
    "This created a .py file that can be seen on the repo. We will now use that generated file to create and parse our messages sent to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38de085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058c8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import proto_message_pb2 as bqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4ad2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = bqm.Job()\n",
    "job.id = -1\n",
    "job.job = \"Not registered job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddea86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "department = bqm.Department()\n",
    "department.id = -1\n",
    "department.department = \"Not registered department\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18b8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "hired_employee = bqm.Hired_employee()\n",
    "hired_employee.id = 2000\n",
    "hired_employee.name = \"Alejandro Rojas\"\n",
    "hired_employee.datetime = datetime.datetime(2023,2,20,19).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "hired_employee.department_id = 5\n",
    "hired_employee.job_id = 179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62afe07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676937912.000034"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " datetime.datetime(2023,2,20,19,5,12,34).timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0887c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x08\\xd0\\x0f\\x12\\x0fAlejandro Rojas\\x1a\\x132023-02-20 19:00:00 \\x05(\\xb3\\x01'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hired_employee.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602003f9",
   "metadata": {},
   "source": [
    "Great! We've created our protobuf messages. \n",
    "\n",
    "Now, following a kind guide written by matthieucham from stax labs, we will try to use a wrapper for our BigQuery calls. Initially try the one suggested by him, if not then tweak it\n",
    "\n",
    "https://dev.to/stack-labs/13-tricks-for-the-new-bigquery-storage-write-api-in-python-296e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec781d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wrapper around BigQuery call.\"\"\"\n",
    "from __future__ import annotations\n",
    "from typing import Any, Iterable\n",
    "import logging\n",
    "from google.cloud import bigquery_storage\n",
    "from google.cloud.bigquery_storage_v1 import exceptions as bqstorage_exceptions\n",
    "\n",
    "from google.cloud.bigquery_storage_v1 import types, writer\n",
    "from google.protobuf import descriptor_pb2\n",
    "from google.protobuf.descriptor import Descriptor\n",
    "\n",
    "\n",
    "\n",
    "class DefaultStreamManager:  # pragma: no cover\n",
    "    \"\"\"Manage access to the _default stream write streams.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        table_path: str,\n",
    "        message_protobuf_descriptor: Descriptor,\n",
    "        bigquery_storage_write_client: bigquery_storage.BigQueryWriteClient,\n",
    "    ):\n",
    "        \"\"\"Init.\"\"\"\n",
    "        self.stream_name = f\"{table_path}/_default\"\n",
    "        self.message_protobuf_descriptor = message_protobuf_descriptor\n",
    "        self.write_client = bigquery_storage_write_client\n",
    "        self.append_rows_stream = None\n",
    "\n",
    "    def _init_stream(self):\n",
    "        \"\"\"Init the underlying stream manager.\"\"\"\n",
    "        # Create a template with fields needed for the first request.\n",
    "        request_template = types.AppendRowsRequest()\n",
    "        # The initial request must contain the stream name.\n",
    "        request_template.write_stream = self.stream_name\n",
    "        # So that BigQuery knows how to parse the serialized_rows, generate a\n",
    "        # protocol buffer representation of our message descriptor.\n",
    "        proto_schema = types.ProtoSchema()\n",
    "        proto_descriptor = descriptor_pb2.DescriptorProto()  # pylint: disable=no-member\n",
    "        self.message_protobuf_descriptor.CopyToProto(proto_descriptor)\n",
    "        proto_schema.proto_descriptor = proto_descriptor\n",
    "        proto_data = types.AppendRowsRequest.ProtoData()\n",
    "        proto_data.writer_schema = proto_schema\n",
    "        request_template.proto_rows = proto_data\n",
    "        # Create an AppendRowsStream using the request template created above.\n",
    "        self.append_rows_stream = writer.AppendRowsStream(\n",
    "            self.write_client, request_template\n",
    "        )\n",
    "\n",
    "    def send_appendrowsrequest(\n",
    "        self, request: types.AppendRowsRequest\n",
    "    ) -> writer.AppendRowsFuture:\n",
    "        \"\"\"Send request to the stream manager. Init the stream manager if needed.\"\"\"\n",
    "        try:\n",
    "            if self.append_rows_stream is None:\n",
    "                self._init_stream()\n",
    "            return self.append_rows_stream.send(request)\n",
    "        except bqstorage_exceptions.StreamClosedError:\n",
    "            # the stream needs to be reinitialized\n",
    "            self.append_rows_stream.close()\n",
    "            self.append_rows_stream = None\n",
    "            raise\n",
    "\n",
    "    # Use as a context manager\n",
    "\n",
    "    def __enter__(self) -> DefaultStreamManager:\n",
    "        \"\"\"Enter the context manager. Return the stream name.\"\"\"\n",
    "        self._init_stream()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"Exit the context manager : close the stream.\"\"\"\n",
    "        if self.append_rows_stream is not None:\n",
    "            # Shutdown background threads and close the streaming connection.\n",
    "            self.append_rows_stream.close()\n",
    "\n",
    "\n",
    "class BigqueryWriteManager:\n",
    "    \"\"\"Encapsulation for bigquery client.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        dataset_id: str,\n",
    "        table_id: str,\n",
    "        bigquery_storage_write_client: bigquery_storage.BigQueryWriteClient,\n",
    "        pb2_descriptor: Descriptor,\n",
    "    ):  # pragma: no cover\n",
    "        \"\"\"Create a BigQueryManager.\"\"\"\n",
    "        self.bigquery_storage_write_client = bigquery_storage_write_client\n",
    "\n",
    "        self.table_path = self.bigquery_storage_write_client.table_path(\n",
    "            project_id, dataset_id, table_id\n",
    "        )\n",
    "        self.pb2_descriptor = pb2_descriptor\n",
    "\n",
    "    def write_rows(self, pb_rows: Iterable[Any]) -> None:\n",
    "        \"\"\"Write data rows.\"\"\"\n",
    "        with DefaultStreamManager(\n",
    "            self.table_path, self.pb2_descriptor, self.bigquery_storage_write_client\n",
    "        ) as target_stream_manager:\n",
    "            proto_rows = types.ProtoRows()\n",
    "            # Create a batch of row data by appending proto2 serialized bytes to the\n",
    "            # serialized_rows repeated field.\n",
    "            for row in pb_rows:\n",
    "                proto_rows.serialized_rows.append(row.SerializeToString())\n",
    "            # Create an append row request containing the rows\n",
    "            request = types.AppendRowsRequest()\n",
    "            proto_data = types.AppendRowsRequest.ProtoData()\n",
    "            proto_data.rows = proto_rows\n",
    "            request.proto_rows = proto_data\n",
    "\n",
    "            future = target_stream_manager.send_appendrowsrequest(request)\n",
    "\n",
    "            # Wait for the append row requests to finish.\n",
    "            future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e3f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
